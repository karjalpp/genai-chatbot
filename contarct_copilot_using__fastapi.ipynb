{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvnLHEl+fmr7dfOQAB5Vkq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karjalpp/genai-chatbot/blob/main/contarct_copilot_using__fastapi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VzVBOt2OJsm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import backoff\n",
        "import PyPDF2\n",
        "import ratelimit\n",
        "from google.api_core import exceptions\n",
        "from tqdm import tqdm\n",
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "import docx2txt\n",
        "from docx import Document\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import FileResponse\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'data-engineering-gcp-practice-a400a0302437.json'\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "vertex_llm_text = VertexAI(model_name=\"text-bison@001\")\n",
        "vertex_embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@001\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
        "\n",
        "CALL_LIMIT = 20  # Number of calls to allow within a period\n",
        "ONE_MINUTE = 60  # One minute in seconds\n",
        "FIVE_MINUTE = 5 * ONE_MINUTE\n",
        "\n",
        "# A function to print a message when the function is retrying\n",
        "def backoff_hdlr(details):\n",
        "    print(\n",
        "        \"Backing off {} seconds after {} tries\".format(\n",
        "            details[\"wait\"], details[\"tries\"]\n",
        "        )\n",
        "    )\n",
        "@backoff.on_exception(  # Retry with exponential backoff strategy when exceptions occur\n",
        "    backoff.expo,\n",
        "    (\n",
        "        exceptions.ResourceExhausted,\n",
        "        ratelimit.RateLimitException,\n",
        "    ),  # Exceptions to retry on\n",
        "    max_time=FIVE_MINUTE,\n",
        "    on_backoff=backoff_hdlr,  # Function to call when retrying\n",
        ")\n",
        "@ratelimit.limits(  # Limit the number of calls to the model per minute\n",
        "    calls=CALL_LIMIT, period=ONE_MINUTE\n",
        ")\n",
        "\n",
        "def model_with_limit_and_backoff(**kwargs):\n",
        "    return generation_model.predict(**kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def Contract_Generation(email,model_with_limit_and_backoff):\n",
        "    prompt= f\"\"\"\n",
        "    Your task is to extract key parameters from email doc.\\\n",
        "    These key parameters are following:\n",
        "    1. Supplier Address\n",
        "    2. Date\n",
        "    3. TERM\n",
        "    4. SOW\n",
        "    5. Deliverables\n",
        "    6. Supplier Name\n",
        "    \\\"\\\"\\\"{email}\\\"\\\"\\\"\n",
        "    \"\"\"\n",
        "    response = model_with_limit_and_backoff(prompt=prompt,max_output_tokens=1024).text\n",
        "    # generation_model.predict(prompt).text\n",
        "    # print(response)\n",
        "    #converting it into dictionary\n",
        "    output = response.replace('Key Parameters:', '')\n",
        "    lines = output.split('\\n')\n",
        "    key_value_pairs = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            # Split the line at the first occurrence of ':'\n",
        "            parts = line.split(': ', 1)\n",
        "            if len(parts) == 2:\n",
        "                key = parts[0].strip()\n",
        "                if key[0].isdigit():\n",
        "                    # Remove the initial number and dot from the key\n",
        "                    key = key.split('. ', 1)[1]\n",
        "                value = parts[1].strip()\n",
        "                key_value_pairs.append((key, value))\n",
        "\n",
        "    output_dict = {f'[{key.upper()}]': value for key, value in key_value_pairs}\n",
        "    # print(output_dict)\n",
        "    file_path = \"MSA template.docx\"\n",
        "    # word_dict = {'[SUPPLIER ADDRESS]': '123 Main Street,\n",
        "    #  Anytown, CA 12345', '[COST]': 'Not mentioned', '[DATE]': 'March 8, 2023', '[TERM]': '1 year'}\n",
        "    document = Document(file_path)\n",
        "    for paragraph in document.paragraphs:\n",
        "        for key, value in output_dict.items():\n",
        "            if key in paragraph.text:\n",
        "                # st.write(key)\n",
        "                # st.write(value)\n",
        "                paragraph.text = paragraph.text.replace(key, value)\n",
        "    # path = f\"Repo/{output_dict['[SUPPLIER NAME]'].split()[0]}.docx\"\n",
        "    # document.save(path)\n",
        "    document.save(f\"email/docx/response.docx\")\n",
        "    return document\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"Hello\": \"World\"}\n",
        "\n",
        "# @app.post('/genai/api/v1/summarization')\n",
        "# async def summarization(file: UploadFile = File(...)):\n",
        "#     file_location = f\"Repo/{file.filename}\"\n",
        "#     with open(file_location, \"wb+\") as file_object:\n",
        "#         file_object.write(file.file.read())\n",
        "\n",
        "#     reader = PyPDF2.PdfReader(os.path.join(\"Repo\",file.filename))\n",
        "#     pages = reader.pages\n",
        "\n",
        "#     initial_prompt_template = \"\"\"\n",
        "#     Taking the following context delimited by triple backquotes into consideration:\n",
        "#     ```{context}```\n",
        "#     Write a concise summary of the following text delimited by triple backquotes which also covers the key points of the text including Scope of Services, Deliverables if available.\n",
        "\n",
        "#     ```{text}```\n",
        "#     CONCISE SUMMARY:\n",
        "#     \"\"\"\n",
        "#     final_prompt_template = \"\"\"\n",
        "#         Write a concise summary of the following text delimited by triple backquotes.\n",
        "#         Return your response in 10 bullet points.\n",
        "\n",
        "#         ```{text}```\n",
        "#         BULLET POINT SUMMARY:\n",
        "#     \"\"\"\n",
        "#     initial_summary = []\n",
        "#     for idx, page in enumerate(tqdm(pages)):\n",
        "#         text = page.extract_text().strip()\n",
        "#         if idx == 0:\n",
        "#             prompt = initial_prompt_template.format(context=\"\", text=text)\n",
        "#         else:\n",
        "#             prompt = initial_prompt_template.format(\n",
        "#                 context=initial_summary[idx - 1], text=text\n",
        "#             )\n",
        "#         summary = model_with_limit_and_backoff(prompt=prompt, max_output_tokens=1024).text\n",
        "#         initial_summary.append(summary)\n",
        "\n",
        "#     def reduce(initial_summary, prompt_template):\n",
        "#         concat_summary = \"\\n\".join(initial_summary)\n",
        "#         prompt = prompt_template.format(text=concat_summary)\n",
        "#         summary = model_with_limit_and_backoff(prompt=prompt, max_output_tokens=1024).text\n",
        "#         return summary\n",
        "\n",
        "#     initial_summary = set(initial_summary)\n",
        "#     summary = reduce(initial_summary, final_prompt_template)\n",
        "#     return {\"summary\": summary}\n",
        "\n",
        "\n",
        "\n",
        "@app.post('/genai/api/v1/singQandA')\n",
        "async def singQandA(input,file: UploadFile = File(...)):\n",
        "    file_location = f\"Repo/{file.filename}\"\n",
        "    with open(file_location, \"wb+\") as file_object:\n",
        "        file_object.write(file.file.read())\n",
        "\n",
        "    reader = PyPDF2.PdfReader(os.path.join(\"Repo\",file.filename))\n",
        "    pages = reader.pages\n",
        "\n",
        "    raw_text = ''\n",
        "    for i, page in enumerate(tqdm(pages)):\n",
        "        content = page.extract_text()\n",
        "        if content:\n",
        "            raw_text += content\n",
        "\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "                                            separator = \"\\n\",\n",
        "                                            chunk_size = 800,\n",
        "                                            chunk_overlap  = 200,\n",
        "                                            length_function = len,\n",
        "                                        )\n",
        "    texts = text_splitter.split_text(raw_text)\n",
        "    embeddings = VertexAIEmbeddings()\n",
        "    document_search = FAISS.from_texts(texts, embeddings)\n",
        "    chain = load_qa_chain(VertexAI(), chain_type=\"stuff\")\n",
        "    query = input\n",
        "    if query:\n",
        "        memory = ConversationBufferWindowMemory(k=3)\n",
        "        docs = document_search.similarity_search(query)\n",
        "        response = chain.run(input_documents=docs, question=query)\n",
        "        return {\"response\":response}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@app.post('/genai/api/v1/contgen')\n",
        "async def contgen(file: UploadFile = File(...)):\n",
        "    file_location = f\"email/{file.filename}\"\n",
        "    with open(file_location, \"wb+\") as file_object:\n",
        "        file_object.write(file.file.read())\n",
        "\n",
        "    email = str(docx2txt.process(os.path.join(\"email\",file.filename)))\n",
        "    document=Contract_Generation(email,model_with_limit_and_backoff)\n",
        "    res=\"\"\n",
        "    for para in document.paragraphs:\n",
        "        res = res + \"\\n\" + para.text\n",
        "        # return para.text\n",
        "    return {'res':res}\n",
        "\n",
        "@app.get(\"/genai/api/v1/download-file\")\n",
        "def download_file():\n",
        "    file_name ='response'\n",
        "    folder_path = r\"email/docx\"\n",
        "    file_location = f'{folder_path}{os.sep}{file_name}.docx'\n",
        "    return FileResponse(file_location, media_type='application/vnd.openxmlformats-officedocument.wordprocessingml.document', filename=file_name)\n",
        "\n",
        "@app.post('/genai/api/v1/multipleQandA')\n",
        "async def multipleQandA(input):\n",
        "    persist_dir = 'Repo/db'\n",
        "    vectordb = Chroma(persist_directory=persist_dir, embedding_function=VertexAIEmbeddings())\n",
        "    memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "    pdf_qa = ConversationalRetrievalChain.from_llm(llm=VertexAI(temperature=0.0,max_output_tokens=1024,model_name=\"text-bison@001\"),\n",
        "                                                    retriever=vectordb.as_retriever(),\n",
        "                                                    memory=memory,\n",
        "                                                    verbose=False\n",
        "                                                   )\n",
        "    # yellow = \"\\033[0;33m\"\n",
        "    # green = \"\\033[0;32m\"\n",
        "    # white = \"\\033[0;39m\"\n",
        "    query = input\n",
        "    if query:\n",
        "        response = pdf_qa({\"question\": query})\n",
        "        return {\"response\":response}\n",
        "\n",
        "\n",
        "@app.get('/genai/api/v1/list_files')\n",
        "async def list_files():\n",
        "    file_location = f\"Repo/summaries\"\n",
        "    try:\n",
        "        files = os.listdir(file_location)\n",
        "        return {'files':files}\n",
        "    except Exception as e:\n",
        "        return {\"error\" : str(e)}\n",
        "\n",
        "\n",
        "@app.post('/genai/api/v1/display_summary')\n",
        "async def display_summary(file_name:str):\n",
        "    path = f\"Repo/summaries/{file_name}.docx\"\n",
        "    document = Document(path)\n",
        "    res=\"\"\n",
        "    for para in document.paragraphs:\n",
        "        res = res + \"\\n\" + para.text\n",
        "    return {'res':res}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/sample_data/docker/docker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "bsNifDxQOfYS",
        "outputId": "6eaf24c4-5990-4707-e71c-739b45b7f1c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "[Errno 20] Not a directory: '/content/sample_data/docker/docker'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ba60d8372df8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/docker/docker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/sample_data/docker/docker'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a new folder named 'new_folder'\n",
        "os.makedirs('new_folder', exist_ok=True)\n"
      ],
      "metadata": {
        "id": "948cCrJ3SAgt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p new_folder\n"
      ],
      "metadata": {
        "id": "IGgRMfWUpCfl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('new_folder')\n",
        "\n"
      ],
      "metadata": {
        "id": "GO9oyZP6pJ0M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write code to a new Python file\n",
        "with open('docker.py', 'w') as file:\n",
        "    file.write('''FROM python:3.10.10-slim-bullseye\n",
        "\n",
        "WORKDIR /\n",
        "\n",
        "COPY docker/requirements.txt ./\n",
        "\n",
        "RUN export HNSWLIB_NO_NATIVE=1\n",
        "RUN apt-get update && apt-get install build-essential -y\n",
        "RUN pip install -r requirements.txt\n",
        "\n",
        "COPY . .\n",
        "\n",
        "ENTRYPOINT [\"uvicorn\", \"test2:app\" , \"--host\" , \"0.0.0.0\" , \"--port\" , \"8080\"]\n",
        "\n",
        "''')\n",
        "\n",
        "# Run the newly created Python file\n",
        "#!python example_script.py\n"
      ],
      "metadata": {
        "id": "qng1vFyDpKoM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('requirements.txt','w') as file:\n",
        "    file.write('''python-multipart\n",
        "deep_translator\n",
        "fastapi\n",
        "uvicorn\n",
        "pyyaml\n",
        "happytransformer\n",
        "google-api-core==2.11.0\n",
        "google-auth==2.16.3\n",
        "google-cloud==0.34.0\n",
        "google-cloud-core==2.3.2\n",
        "googleapis-common-protos==1.59.0\n",
        "scikit-learn\n",
        "pandas\n",
        "chromadb\n",
        "langchain==0.0.205\n",
        "google-cloud-aiplatform==1.26.0\n",
        "pypdf2\n",
        "docx2txt\n",
        "tqdm\n",
        "requests\n",
        "ratelimit\n",
        "python-docx\n",
        "faiss-cpu==1.7.4''')"
      ],
      "metadata": {
        "id": "rGxmIWIbpiWY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('dockerignore','w') as file:\n",
        "  file.write('''.git\n",
        ".gitignore\n",
        "Dockerfile\n",
        "README.md\n",
        "node_modules\n",
        "npm-debug.log\n",
        "build/\n",
        "tmp/\n",
        ".env\n",
        "''')"
      ],
      "metadata": {
        "id": "rk10ce8GqdKO"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}